{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Big Data SaaS Management Demonstration \u00b6 Update 1/04/2023 Introduction \u00b6 As an ISV delivering Big Data platform for their on-premises customers, AnyCompany wants to move to a SaaS model. They designed a new architecture with multi-tenant support. They selected the multi-tebancy 'bridge' pattern to support their different customer's requirements: Figure 1: Multi-tenant patterns The Bridge pattern means some customers will run their big data workload on a shared cluster, while others will be fully isolated at the cluster level. A fully isolated solution will be with the Silo pattern, where customers run on their own account and get a copy of the SaaS vendor software stack. On the right side the Pool pattern is mostly used during product evalutio, tutorial, getting started with the SaaS product, where all the tenants are in the same cluster resource. Even in pool or bridge pattern the data are isolated. The current AnyCompany's software stack for the big data processing looks as in the figure below: Figure 2: Big-data simple view A control plane manages the job submission into a cluster of worker nodes. Each node has an executor that runs jobs. A job includes a set of step for data loading, transformation, processing and persistence of the results. Once jobs are terminated resources can be reallocated. The distributed file storage supports two use cases, the long term persistence, highly available and scalable of the data lake, and local NFS cluster to support the job execution data caching and snapshots. The new SaaS platform includes a set of microservices to register tenant, to manage account, billing, and specific components which support data catalog, and coordinate big data batch execution. The following diagram is the outcome of the Dicovery workshop hold early December 2022 with the AnyCompany architects to illustrates a high-level system context to support a MVP or a demonstration mapped to AWS services: Figure 3: Discovery outcome All the platform components are generating a lot of metadata about tenant, user's activities and job submissions, and we want to propose to extend their SaaS architecture to leverage AWS services such as Kinesis, SageMaker, Quicksight to monitor their users activities and assess the risk of churn bof leaving platform. Business Motivations \u00b6 The following business questions may be answered by using the new analytics platform: How often tenant register and user login, and work on data lake and then submit jobs? Which customers are not doing a lot of activities after logging? What is the size of their data set? How many batches are run per customer, per day? Can we identify the customers doing very minimum? Demonstration Scope \u00b6 From a demonstration point of view, we want to address the data pipeline, the Scoring service integration into real-time event processing, and the dashboarding. The following figure illustrates the scope of the demonstration in term of components involved: Figure 4: Component View The control plane will be supported by two Java Microprofile services that will persist state in different database type (dynamoDB for the job manager, and RDS Postgrseql for Tenant Manager) and generate events to Kinesis Data Streams. Tenant Manager is a basic Java Quarkus, JPA with Panache and RDS Postgresql DB to persist Tenant, Users, billing tables. Job Manager is also a Java Quarkus app, used to simulate Batch Big data Job creation to the Big Data platform. Those two services run on AWS EKS, the Kubernetes cluster managed services. The value of running the microservices on EKS is to support a serverless deployment model, where Kubernetes cluster scales the K8s control plane across multiple AZs. The following figure demonstrates a classical EKS cluster deployment inside of an AWS region. As an elastic platform, we can add nodes to the EKS cluster, and each node run hundred of containers: Figure 5: Reference Architecture for EKS deployment No need to install, operate and maintain k8s cluster. It automatically scales control plane instances based on load, detects and replaces unhealthy containers. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers. EKS uses IAM to provide authentication to your Kubernetes cluster, and k8s RBAC for authorization. See this note for details about running the SaaS solution and customer data plane in a multi-tenant way to support the bridge pattern . As en event-driven solution we want to have events ingected to Kinesis Data Streams so we can plug real-time analytics (running in Kinesis Data Analytics) to be able to answer some of the questions asked by the business users. We will use Infrastructure as Code as much as possible to automate the provisioning of services and resources. Components list from figure 4 \u00b6 Amazon SageMaker to support the development, deployment and hosting of the risk of churn scoring: The description of the model is in this note . Dashboard in Amazon QuickSight : is a simple dashboard to illustrate some of the metrics as defined by the requirements listed above: See the note for implementatiopn details. API Gateway and Lambda function to proxy/ facade SageMaker scoring service, see this note . Kinesis Data Streams to persist and distribute events generated by the different components of the solution. See this note for configuration . Real-time analytics using Kinesis Data Analytics, the details for the implementation and deployment are in this note . AWS S3 : is used by the SaaS's customers to support persistence of their data, and it is used in our solutions to support two use cases: Model development, with persistence of training, validation and test sets, and the model built. To persist the aggregates events, output of the data streams processing, and input for the QuickSight dahboard. To be continued: DynamoDB RDS table Tenant manager Microservice Job Manager Microservice Deeper Dive \u00b6 Below are the sources of information I used to develop this solution: Real Time ML inference on Streaming Data - Lab Sagemaker building your own model. Streaming Data Solution for Amazon Kinesis Kinesis Data Analytics Java Samples Github AWS CDK AWS SDK Java Linear Learner","title":"Introduction"},{"location":"#big-data-saas-management-demonstration","text":"Update 1/04/2023","title":"Big Data SaaS Management Demonstration"},{"location":"#introduction","text":"As an ISV delivering Big Data platform for their on-premises customers, AnyCompany wants to move to a SaaS model. They designed a new architecture with multi-tenant support. They selected the multi-tebancy 'bridge' pattern to support their different customer's requirements: Figure 1: Multi-tenant patterns The Bridge pattern means some customers will run their big data workload on a shared cluster, while others will be fully isolated at the cluster level. A fully isolated solution will be with the Silo pattern, where customers run on their own account and get a copy of the SaaS vendor software stack. On the right side the Pool pattern is mostly used during product evalutio, tutorial, getting started with the SaaS product, where all the tenants are in the same cluster resource. Even in pool or bridge pattern the data are isolated. The current AnyCompany's software stack for the big data processing looks as in the figure below: Figure 2: Big-data simple view A control plane manages the job submission into a cluster of worker nodes. Each node has an executor that runs jobs. A job includes a set of step for data loading, transformation, processing and persistence of the results. Once jobs are terminated resources can be reallocated. The distributed file storage supports two use cases, the long term persistence, highly available and scalable of the data lake, and local NFS cluster to support the job execution data caching and snapshots. The new SaaS platform includes a set of microservices to register tenant, to manage account, billing, and specific components which support data catalog, and coordinate big data batch execution. The following diagram is the outcome of the Dicovery workshop hold early December 2022 with the AnyCompany architects to illustrates a high-level system context to support a MVP or a demonstration mapped to AWS services: Figure 3: Discovery outcome All the platform components are generating a lot of metadata about tenant, user's activities and job submissions, and we want to propose to extend their SaaS architecture to leverage AWS services such as Kinesis, SageMaker, Quicksight to monitor their users activities and assess the risk of churn bof leaving platform.","title":"Introduction"},{"location":"#business-motivations","text":"The following business questions may be answered by using the new analytics platform: How often tenant register and user login, and work on data lake and then submit jobs? Which customers are not doing a lot of activities after logging? What is the size of their data set? How many batches are run per customer, per day? Can we identify the customers doing very minimum?","title":"Business Motivations"},{"location":"#demonstration-scope","text":"From a demonstration point of view, we want to address the data pipeline, the Scoring service integration into real-time event processing, and the dashboarding. The following figure illustrates the scope of the demonstration in term of components involved: Figure 4: Component View The control plane will be supported by two Java Microprofile services that will persist state in different database type (dynamoDB for the job manager, and RDS Postgrseql for Tenant Manager) and generate events to Kinesis Data Streams. Tenant Manager is a basic Java Quarkus, JPA with Panache and RDS Postgresql DB to persist Tenant, Users, billing tables. Job Manager is also a Java Quarkus app, used to simulate Batch Big data Job creation to the Big Data platform. Those two services run on AWS EKS, the Kubernetes cluster managed services. The value of running the microservices on EKS is to support a serverless deployment model, where Kubernetes cluster scales the K8s control plane across multiple AZs. The following figure demonstrates a classical EKS cluster deployment inside of an AWS region. As an elastic platform, we can add nodes to the EKS cluster, and each node run hundred of containers: Figure 5: Reference Architecture for EKS deployment No need to install, operate and maintain k8s cluster. It automatically scales control plane instances based on load, detects and replaces unhealthy containers. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers. EKS uses IAM to provide authentication to your Kubernetes cluster, and k8s RBAC for authorization. See this note for details about running the SaaS solution and customer data plane in a multi-tenant way to support the bridge pattern . As en event-driven solution we want to have events ingected to Kinesis Data Streams so we can plug real-time analytics (running in Kinesis Data Analytics) to be able to answer some of the questions asked by the business users. We will use Infrastructure as Code as much as possible to automate the provisioning of services and resources.","title":"Demonstration Scope"},{"location":"#components-list-from-figure-4","text":"Amazon SageMaker to support the development, deployment and hosting of the risk of churn scoring: The description of the model is in this note . Dashboard in Amazon QuickSight : is a simple dashboard to illustrate some of the metrics as defined by the requirements listed above: See the note for implementatiopn details. API Gateway and Lambda function to proxy/ facade SageMaker scoring service, see this note . Kinesis Data Streams to persist and distribute events generated by the different components of the solution. See this note for configuration . Real-time analytics using Kinesis Data Analytics, the details for the implementation and deployment are in this note . AWS S3 : is used by the SaaS's customers to support persistence of their data, and it is used in our solutions to support two use cases: Model development, with persistence of training, validation and test sets, and the model built. To persist the aggregates events, output of the data streams processing, and input for the QuickSight dahboard. To be continued: DynamoDB RDS table Tenant manager Microservice Job Manager Microservice","title":"Components list from figure 4"},{"location":"#deeper-dive","text":"Below are the sources of information I used to develop this solution: Real Time ML inference on Streaming Data - Lab Sagemaker building your own model. Streaming Data Solution for Amazon Kinesis Kinesis Data Analytics Java Samples Github AWS CDK AWS SDK Java Linear Learner","title":"Deeper Dive"},{"location":"api-lambda/","text":"Anti-corruption layer with Lambda function \u00b6 As illustrated by the architecture diagram, SageMaker is not directly accessed from the real-time analytics component but via an API Gateway and a lambda function: The approach is to implement an anti-corruption layer where the data model of SageMaker is not corrupting the Tenant domain. This is a classical pattern in Domain Driven Design, and it makes sense in the context of this demonstration to illustrate this approach. Lambda \u00b6 The goal of the lambda function is to call a scoring model deployed in SageMaker runtime, and do data transformation from Company business entity represented as JSON document and the csv payload expected by the SageMaker service. The code is defined in lambda-handler.py . The deployment can be fully automated with CDK. For this lambda, to be able to access SageMaker, we need to define a IAM Role with SageMaker invokeEndpoint permission, { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"sagemaker:InvokeEndpoint\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } and to get the function being able to generate logs ( AWSLambdaBasicExecutionRole ). { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:403993201276:log-group:/aws/lambda/mapper-SMInvokeEndpoint:*\" ], \"Effect\" : \"Allow\" } ] } The Handler code is in the python program: lambda-handler.py API Gateway \u00b6 It is a fully managed service to define, deploy, monitor APIs: HTTP, REST, WebSocket. It forms the app-facing part of AWS Serverless. It supports hundred of thousands of concurrent API calls. We are defining the API via CDK, see the apigw_lambda_cdk.py Here is an example of the API defined in API Gateway to proxy SageMaker. We can test the API and SageMaker using the following payload: { \"companyID\" : \"comp_4\" , \"revenu\" : 99420 , \"industry\" : \"travel\" , \"employee\" : 4635 , \"job30\" : 10 , \"job90\" : 100 , \"monthlyFee\" : 400 , \"totalFee\" : 1200 , \"riskOfChurn\" : false } The result may look like: { \"churn\" : true } To get the API URL endpoint, go to API Gateway, Stages environment and select the URL path: this URL needs to be specified in the Kinesis Data Analytics parameters.","title":"API Gateway - Lambda"},{"location":"api-lambda/#anti-corruption-layer-with-lambda-function","text":"As illustrated by the architecture diagram, SageMaker is not directly accessed from the real-time analytics component but via an API Gateway and a lambda function: The approach is to implement an anti-corruption layer where the data model of SageMaker is not corrupting the Tenant domain. This is a classical pattern in Domain Driven Design, and it makes sense in the context of this demonstration to illustrate this approach.","title":"Anti-corruption layer with Lambda function"},{"location":"api-lambda/#lambda","text":"The goal of the lambda function is to call a scoring model deployed in SageMaker runtime, and do data transformation from Company business entity represented as JSON document and the csv payload expected by the SageMaker service. The code is defined in lambda-handler.py . The deployment can be fully automated with CDK. For this lambda, to be able to access SageMaker, we need to define a IAM Role with SageMaker invokeEndpoint permission, { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"sagemaker:InvokeEndpoint\" ], \"Resource\" : \"*\" , \"Effect\" : \"Allow\" } ] } and to get the function being able to generate logs ( AWSLambdaBasicExecutionRole ). { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Action\" : [ \"logs:CreateLogGroup\" , \"logs:CreateLogStream\" , \"logs:PutLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:403993201276:log-group:/aws/lambda/mapper-SMInvokeEndpoint:*\" ], \"Effect\" : \"Allow\" } ] } The Handler code is in the python program: lambda-handler.py","title":"Lambda"},{"location":"api-lambda/#api-gateway","text":"It is a fully managed service to define, deploy, monitor APIs: HTTP, REST, WebSocket. It forms the app-facing part of AWS Serverless. It supports hundred of thousands of concurrent API calls. We are defining the API via CDK, see the apigw_lambda_cdk.py Here is an example of the API defined in API Gateway to proxy SageMaker. We can test the API and SageMaker using the following payload: { \"companyID\" : \"comp_4\" , \"revenu\" : 99420 , \"industry\" : \"travel\" , \"employee\" : 4635 , \"job30\" : 10 , \"job90\" : 100 , \"monthlyFee\" : 400 , \"totalFee\" : 1200 , \"riskOfChurn\" : false } The result may look like: { \"churn\" : true } To get the API URL endpoint, go to API Gateway, Stages environment and select the URL path: this URL needs to be specified in the Kinesis Data Analytics parameters.","title":"API Gateway"},{"location":"cdk/","text":"Infrastructure as Code \u00b6 To use an infrastructure as code, we use CDK to create all the needed services and resources. The AWS CDK revolves around a fundamental building block called a construct. These constructs have three abstraction levels: L1 \u2013 A one-to-one mapping to AWS CloudFormation L2 \u2013 An intent-based API L3 \u2013 A high-level pattern The solution CDK is under setup/saas-solution-cdk folder. The CDK creates the following elements: VPC with private and public subnets RDS Postgresql EKS cluster API Gateway Lambda function Kinesis Data Streams Kinesis Data Analytics Deploy \u00b6 Under setup/saas-solution-cdk , start a python environment with AWS and CDK CLIs: ./startPythonDocker.sh In the shell use: cdk synth # The first time you run CDK in your account in the selected region do: cdk bootstrap cdk deploy Clean up cdk destroy Explanations \u00b6 VPC \u00b6 The declaration builds a VPC with 2 public and 2 private subnets: vpc = aws_ec2 . Vpc ( self , \"VPC\" , max_azs = 2 , ip_addresses = aws_ec2 . IpAddresses . cidr ( cidr ), nat_gateways = 2 , enable_dns_hostnames = True , enable_dns_support = True , subnet_configuration = [ aws_ec2 . SubnetConfiguration ( name = \"public\" , subnet_type = aws_ec2 . SubnetType . PUBLIC , cidr_mask = 24 ), aws_ec2 . SubnetConfiguration ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS , name = \"private\" , cidr_mask = 24 ) # could be /16 to have more instances, but this is a demo scope. ] ) The result: There are also two NAT gateways for outbound traffic and one Internet Gateway attached to the VPC. Five route tables are created, one for each subnet: for public subnets each route defines all internet inbound traffic to go to Internet Gateway, local on CICD defines (10.10.0.0/16), use local network, and outbound traffic from private subnets goes to NAT gateways. RDS CDK \u00b6 Once the VPC is created, we want the database to be in private network, but accessible in public so we can use pgadmin to see the schema. This is for development purpose. The declaration looks like: postgres = aws_rds . DatabaseInstance ( self , \"PostgresqlInstance\" , database_name = \"tenantdb\" , engine = aws_rds . DatabaseInstanceEngine . postgres ( version = aws_rds . PostgresEngineVersion . VER_14_5 ), vpc_subnets = aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS ), vpc = self . vpc , port = 5432 , removal_policy = RemovalPolicy . DESTROY , deletion_protection = False , max_allocated_storage = 200 , publicly_accessible = True ) The cloudformation will create a secret in AWS Secret Manager with the credential, hostname... to access postgres database. And the security group needs to be modified to add inbound rules for port 5432 from any IP v4 addresses. RDS Connection setup documentation EKS cluster \u00b6 The declaration to create the EKS cluster in the same VPC is: cluster = aws_eks . Cluster ( self , 'demo-cluster' , masters_role = self . eks_admin_role , vpc = self . vpc , default_capacity = 2 , vpc_subnets = [ aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS )], version = aws_eks . KubernetesVersion . V1_24 , output_cluster_name = True ) Deeper dive \u00b6 See python CDK API","title":"Infrastructure as code"},{"location":"cdk/#infrastructure-as-code","text":"To use an infrastructure as code, we use CDK to create all the needed services and resources. The AWS CDK revolves around a fundamental building block called a construct. These constructs have three abstraction levels: L1 \u2013 A one-to-one mapping to AWS CloudFormation L2 \u2013 An intent-based API L3 \u2013 A high-level pattern The solution CDK is under setup/saas-solution-cdk folder. The CDK creates the following elements: VPC with private and public subnets RDS Postgresql EKS cluster API Gateway Lambda function Kinesis Data Streams Kinesis Data Analytics","title":"Infrastructure as Code"},{"location":"cdk/#deploy","text":"Under setup/saas-solution-cdk , start a python environment with AWS and CDK CLIs: ./startPythonDocker.sh In the shell use: cdk synth # The first time you run CDK in your account in the selected region do: cdk bootstrap cdk deploy Clean up cdk destroy","title":"Deploy"},{"location":"cdk/#explanations","text":"","title":"Explanations"},{"location":"cdk/#vpc","text":"The declaration builds a VPC with 2 public and 2 private subnets: vpc = aws_ec2 . Vpc ( self , \"VPC\" , max_azs = 2 , ip_addresses = aws_ec2 . IpAddresses . cidr ( cidr ), nat_gateways = 2 , enable_dns_hostnames = True , enable_dns_support = True , subnet_configuration = [ aws_ec2 . SubnetConfiguration ( name = \"public\" , subnet_type = aws_ec2 . SubnetType . PUBLIC , cidr_mask = 24 ), aws_ec2 . SubnetConfiguration ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS , name = \"private\" , cidr_mask = 24 ) # could be /16 to have more instances, but this is a demo scope. ] ) The result: There are also two NAT gateways for outbound traffic and one Internet Gateway attached to the VPC. Five route tables are created, one for each subnet: for public subnets each route defines all internet inbound traffic to go to Internet Gateway, local on CICD defines (10.10.0.0/16), use local network, and outbound traffic from private subnets goes to NAT gateways.","title":"VPC"},{"location":"cdk/#rds-cdk","text":"Once the VPC is created, we want the database to be in private network, but accessible in public so we can use pgadmin to see the schema. This is for development purpose. The declaration looks like: postgres = aws_rds . DatabaseInstance ( self , \"PostgresqlInstance\" , database_name = \"tenantdb\" , engine = aws_rds . DatabaseInstanceEngine . postgres ( version = aws_rds . PostgresEngineVersion . VER_14_5 ), vpc_subnets = aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS ), vpc = self . vpc , port = 5432 , removal_policy = RemovalPolicy . DESTROY , deletion_protection = False , max_allocated_storage = 200 , publicly_accessible = True ) The cloudformation will create a secret in AWS Secret Manager with the credential, hostname... to access postgres database. And the security group needs to be modified to add inbound rules for port 5432 from any IP v4 addresses. RDS Connection setup documentation","title":"RDS CDK"},{"location":"cdk/#eks-cluster","text":"The declaration to create the EKS cluster in the same VPC is: cluster = aws_eks . Cluster ( self , 'demo-cluster' , masters_role = self . eks_admin_role , vpc = self . vpc , default_capacity = 2 , vpc_subnets = [ aws_ec2 . SubnetSelection ( subnet_type = aws_ec2 . SubnetType . PRIVATE_WITH_EGRESS )], version = aws_eks . KubernetesVersion . V1_24 , output_cluster_name = True )","title":"EKS cluster"},{"location":"cdk/#deeper-dive","text":"See python CDK API","title":"Deeper dive"},{"location":"dashboard/","text":"Business DashBoard with QuickSight \u00b6 Business Insight Queries \u00b6 Recall that the goals for this dashboard is to be able to answer to following questions: How often tenants work on data lake and then submit jobs? Which customers are not doing a lot of activities after logging? What is the size of their data set? How many batches are run per customer, per day? Why AWS QuickSight \u00b6 Amazon QuickSight is a very efficient platform to integrate with the different AWS services to get data from, build dashboard with a lot of predefined visualization constructs. It integrates with RDS, Aurora, Athena, S3, RedShift, OpenSearch, Timestream, with Saleforce, Jira... and with any JDBC compliant database. It can import CSV, XLSX, JSON, TSV files and log files. Data can be cached in memory with the SPICE engine, to reduce the latency to data access. Dashboards can be embedded inside any web app. Multiple users can collaborate on the development of dashboards, data sets can be shared between project. Finally it supports, end user to engage with conversational questions about the data by using the Amazon QuickSight Q ML-powered engine to receive relevant visualizations. End Result \u00b6 The following first sheet of the dashboard includes visualization about all the AnyCompany's customers, with data per industry and then monthly revenue or total revenue per customer: While the second sheet presents the job related number per day with some insight about the companies running the less jobs: Dashboard Link Demonstration \u00b6 Go to the portal Explain each widgets Explain navigation: we can drill down from industry to company. Architecture \u00b6 Business functions are illustrated in the figure below, with the SaaS big-data platform used by the SaaS customer's Data Sciences to run big data processing. The job metadata and user's click streams are ingected to the SaaS data lake on which intelligent queries serve a BI dashboard. If we take this business architecture and maps it with to AWS services we have the following figure: (See detail in the design section ) Build the dashboard \u00b6 Start QuickSight Modify policy so QuickSight can access the bucket where Stream Analytics output its job's outcome. Define a manifest file for accessing the S3 bucket and folders (See these manifests as source). The files need to have the same structure If the upload of the manifest fails with a criptic message, see this note Create a Dataset from S3 bucket customer file and one Dataset for jobs file Change the Type of Date from String to Date Add one Analysis and be sure to add the second dataset. Add visualization","title":"Business Dashboard"},{"location":"dashboard/#business-dashboard-with-quicksight","text":"","title":"Business DashBoard with QuickSight"},{"location":"dashboard/#business-insight-queries","text":"Recall that the goals for this dashboard is to be able to answer to following questions: How often tenants work on data lake and then submit jobs? Which customers are not doing a lot of activities after logging? What is the size of their data set? How many batches are run per customer, per day?","title":"Business Insight Queries"},{"location":"dashboard/#why-aws-quicksight","text":"Amazon QuickSight is a very efficient platform to integrate with the different AWS services to get data from, build dashboard with a lot of predefined visualization constructs. It integrates with RDS, Aurora, Athena, S3, RedShift, OpenSearch, Timestream, with Saleforce, Jira... and with any JDBC compliant database. It can import CSV, XLSX, JSON, TSV files and log files. Data can be cached in memory with the SPICE engine, to reduce the latency to data access. Dashboards can be embedded inside any web app. Multiple users can collaborate on the development of dashboards, data sets can be shared between project. Finally it supports, end user to engage with conversational questions about the data by using the Amazon QuickSight Q ML-powered engine to receive relevant visualizations.","title":"Why AWS QuickSight"},{"location":"dashboard/#end-result","text":"The following first sheet of the dashboard includes visualization about all the AnyCompany's customers, with data per industry and then monthly revenue or total revenue per customer: While the second sheet presents the job related number per day with some insight about the companies running the less jobs: Dashboard Link","title":"End Result"},{"location":"dashboard/#demonstration","text":"Go to the portal Explain each widgets Explain navigation: we can drill down from industry to company.","title":"Demonstration"},{"location":"dashboard/#architecture","text":"Business functions are illustrated in the figure below, with the SaaS big-data platform used by the SaaS customer's Data Sciences to run big data processing. The job metadata and user's click streams are ingected to the SaaS data lake on which intelligent queries serve a BI dashboard. If we take this business architecture and maps it with to AWS services we have the following figure: (See detail in the design section )","title":"Architecture"},{"location":"dashboard/#build-the-dashboard","text":"Start QuickSight Modify policy so QuickSight can access the bucket where Stream Analytics output its job's outcome. Define a manifest file for accessing the S3 bucket and folders (See these manifests as source). The files need to have the same structure If the upload of the manifest fails with a criptic message, see this note Create a Dataset from S3 bucket customer file and one Dataset for jobs file Change the Type of Date from String to Date Add one Analysis and be sure to add the second dataset. Add visualization","title":"Build the dashboard"},{"location":"design/","text":"Architecture and Design \u00b6 This section addresses major architecture decisions and design choices: Real-time streaming and analytics API gateway and ML predictive scoring. Business intelligence on data at rest. Component view \u00b6 As presented in the introduction we have the following components in scope for this demonstration: Figure 1: Solution Component View The Dashboard to present business insight on the execusion of the SaaS business. It uses AWS QuickSight. SageMaker to support development of scoring model and runtime deployment . API Gateway and Lambda to proxy the predictive service and do data transformation. Real-time data analytics to do stateful processing. Domain Driven Design \u00b6 Event Definition \u00b6 We need to track the following events: tenant created: company name and industry, sentiment about the company user added, user deleted user login, user logoff, user session timeoff jobSubmitted, jobTerminated, jobCancelled Concentrates those events into kinesis streams: companies, jobs Keep data for 24 hours Move data for long term persistence to S3, one bucket per companies Business entities \u00b6 We define Tenant as part of the tenant manager microservice, which will be considered as company in the risk scoring domain Job is the entity to present big data batch processing. EKS cluster creation and solution deployment \u00b6 The microservices are deployed to EKS. We could have added Kafka as a middleware, deployed in EKS with the Strimzi operator, but the goal of this demonstration is to integrate with Kinesis Data Streams and Kinesis Data Analytics. Kinesis Data Streams \u00b6 Why \u00b6 This is a managed service for pub/sub streaming data. It is a distributed data stream into Shards for parallel processing. Producer sends message with Partition Key and a throughput of 1 Mb/s or 1000 msg /s per Shard. A sequence number is added to the message to note where the message is in the Shard. Retention from 1 to 365 days. Capable to replay the messages. Immutable records, not deleted by applications. Message in a shard, can share partition key, and keep ordering. Producer can use SDK, or Kinesis Producer Library (KPL) or being a Kinesis agent. Consumer may use SDK and Kinesis Client Library (KCL), or being one of the managed services like: Lambda, Kinesis Data Firehose, Kinesis Data Analytics. For consuming side, each Shard gets 2MB/s out. It uses enhanced fan-out if we have multiple consumers retrieving data from a stream in parallel. This throughput automatically scales with the number of shards in a stream. Pricing is per Shard provisioned per hour. The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity. Deployment \u00b6 The CDK app under setup/kinesis-cdk folder defines the following components: And the steps to start them are: Start the CDK app to create CloudFormation template and run it cd setup/kinesis-cdk cdk deploy Verify the stream is created aws kinesis list-streams aws kinesis describe-stream-summary --stream-name bg-jobs { \"StreamDescriptionSummary\" : { \"StreamName\" : \"bg-jobs\" , \"StreamARN\" : \"arn:aws:kinesis:us-west-2:403993201276:stream/bg-jobs\" , \"StreamStatus\" : \"ACTIVE\" , \"StreamModeDetails\" : { \"StreamMode\" : \"PROVISIONED\" } , \"RetentionPeriodHours\" : 24 , \"StreamCreationTimestamp\" : \"2022-12-20T21:11:04-08:00\" , \"EnhancedMonitoring\" : [ { \"ShardLevelMetrics\" : [] } ] , \"EncryptionType\" : \"KMS\" , \"KeyId\" : \"alias/aws/kinesis\" , \"OpenShardCount\" : 1 , \"ConsumerCount\" : 0 } } Put records Validate Records, even if data is encrypted by default: SHARD_ITERATOR = $( aws kinesis get-shard-iterator --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON --stream-name bg-jobs --query 'ShardIterator' ) aws kinesis get-records --shard-iterator $SHARD_ITERATOR Kinesis Data Analytics \u00b6 This is a managed service to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. It can consume records from different source, and in this demonstration we use Kinesis Data Streams. The underlying architecture consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers. The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager To support the execution of Flink job, KDA provides resources using units called Kinesis Processing Units (KPUs). One KPU represents the following system resources: One CPU core 4 GB of memory, of which one GB is native memory and three GB are heap memory 50 GB of disk space The number of KPU = Parallelism parameter / ParallelismPerKPU parameter. When to choose what \u00b6 As Apache Flink is an open-source project, it is possible to deploy it in a Kubernetes cluster, using Flink operator. This will bring you with the most flexible solution as you can select the underlying EC2 instances needed, to optimize your cost. Also you will have fine-grained control over cluster settings, debugging tools and monitoring. While Kinesis Data Analytics helps you to focus on the application logic, which is not simple programming experience, as stateful processing is challenginf, there is no management of infrastructure, monitoring, auto scaling and high availability integrated in the service. In addition to the AWS integrations, the Kinesis Data Analytics libraries include more than 10 Apache Flink connectors and the ability to build custom integrations. Implementation Details \u00b6 See this dedicated note QuickSight Integration Design \u00b6 We need to represent the following metrics within QuickSight: tenants, # of users job submitted over time per tenant last activity date per tenant number of job in last 30 and 90 days The Dashboard is supported by Amazon QuickSight, which helps us to develop Analysis from different datasources, with drill down capabilities. The datasources are in S3 bucket with files that are continuously updated by the Data Streaming and Analtics components. In QuiskSight data sources are mapped to data sets, and we can apply data transformation for a better usage in analysis and visualisation. Here is an example of User Interface constructs: For some implementation details see this note . In the future we can automate the deployment of this dashboard by using CloudFormation. See this note . API Gateway And Lambda Function for SageMaker \u00b6 As introduced in figure 1, API Gateway is deployed to expose REST API, and specially one supported by a Lambda function in backend to proxy SageMaker. The goal here is to add an anti-corruption layer into the domain of data streaming which most of the time represent business events while SageMaker use csv type of input. There is a cloudFormation template defined in setup/cloudformation folder named APIGW-Lambda.yaml to declare API Gateway, role, and Lambda function. The following command create this stack in the connected Region: aws cloudformation create-stack --stack-name apigw-lambda-sm --template-body file://APIGW-Lambda.yaml --parameters ParameterKey = sagemakerEndpoint,ParameterValue = linear-learner-2022-12-22-23-12-40-646 --capabilities CAPABILITY_NAMED_IAM # Result looks like: { \"StackId\" : \"arn:aws:cloudformation:us-west-2:......:stack/apigw-lambda-sm/5041dce0-.....8258-11ed-8ddc-06ec61b22a8d\" } We can access the Lambda and perform a smoke test to verify we reach the SageMaker endpoint.","title":"Architecture & Design"},{"location":"design/#architecture-and-design","text":"This section addresses major architecture decisions and design choices: Real-time streaming and analytics API gateway and ML predictive scoring. Business intelligence on data at rest.","title":"Architecture and Design"},{"location":"design/#component-view","text":"As presented in the introduction we have the following components in scope for this demonstration: Figure 1: Solution Component View The Dashboard to present business insight on the execusion of the SaaS business. It uses AWS QuickSight. SageMaker to support development of scoring model and runtime deployment . API Gateway and Lambda to proxy the predictive service and do data transformation. Real-time data analytics to do stateful processing.","title":"Component view"},{"location":"design/#domain-driven-design","text":"","title":"Domain Driven Design"},{"location":"design/#event-definition","text":"We need to track the following events: tenant created: company name and industry, sentiment about the company user added, user deleted user login, user logoff, user session timeoff jobSubmitted, jobTerminated, jobCancelled Concentrates those events into kinesis streams: companies, jobs Keep data for 24 hours Move data for long term persistence to S3, one bucket per companies","title":"Event Definition"},{"location":"design/#business-entities","text":"We define Tenant as part of the tenant manager microservice, which will be considered as company in the risk scoring domain Job is the entity to present big data batch processing.","title":"Business entities"},{"location":"design/#eks-cluster-creation-and-solution-deployment","text":"The microservices are deployed to EKS. We could have added Kafka as a middleware, deployed in EKS with the Strimzi operator, but the goal of this demonstration is to integrate with Kinesis Data Streams and Kinesis Data Analytics.","title":"EKS cluster creation and solution deployment"},{"location":"design/#kinesis-data-streams","text":"","title":"Kinesis Data Streams"},{"location":"design/#why","text":"This is a managed service for pub/sub streaming data. It is a distributed data stream into Shards for parallel processing. Producer sends message with Partition Key and a throughput of 1 Mb/s or 1000 msg /s per Shard. A sequence number is added to the message to note where the message is in the Shard. Retention from 1 to 365 days. Capable to replay the messages. Immutable records, not deleted by applications. Message in a shard, can share partition key, and keep ordering. Producer can use SDK, or Kinesis Producer Library (KPL) or being a Kinesis agent. Consumer may use SDK and Kinesis Client Library (KCL), or being one of the managed services like: Lambda, Kinesis Data Firehose, Kinesis Data Analytics. For consuming side, each Shard gets 2MB/s out. It uses enhanced fan-out if we have multiple consumers retrieving data from a stream in parallel. This throughput automatically scales with the number of shards in a stream. Pricing is per Shard provisioned per hour. The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within your data stream to provide enough capacity.","title":"Why"},{"location":"design/#deployment","text":"The CDK app under setup/kinesis-cdk folder defines the following components: And the steps to start them are: Start the CDK app to create CloudFormation template and run it cd setup/kinesis-cdk cdk deploy Verify the stream is created aws kinesis list-streams aws kinesis describe-stream-summary --stream-name bg-jobs { \"StreamDescriptionSummary\" : { \"StreamName\" : \"bg-jobs\" , \"StreamARN\" : \"arn:aws:kinesis:us-west-2:403993201276:stream/bg-jobs\" , \"StreamStatus\" : \"ACTIVE\" , \"StreamModeDetails\" : { \"StreamMode\" : \"PROVISIONED\" } , \"RetentionPeriodHours\" : 24 , \"StreamCreationTimestamp\" : \"2022-12-20T21:11:04-08:00\" , \"EnhancedMonitoring\" : [ { \"ShardLevelMetrics\" : [] } ] , \"EncryptionType\" : \"KMS\" , \"KeyId\" : \"alias/aws/kinesis\" , \"OpenShardCount\" : 1 , \"ConsumerCount\" : 0 } } Put records Validate Records, even if data is encrypted by default: SHARD_ITERATOR = $( aws kinesis get-shard-iterator --shard-id shardId-000000000000 --shard-iterator-type TRIM_HORIZON --stream-name bg-jobs --query 'ShardIterator' ) aws kinesis get-records --shard-iterator $SHARD_ITERATOR","title":"Deployment"},{"location":"design/#kinesis-data-analytics","text":"This is a managed service to transform and analyze streaming data in real time using Apache Flink, an open-source framework and engine for processing data streams. It can consume records from different source, and in this demonstration we use Kinesis Data Streams. The underlying architecture consists of a Job Manager and n Task Managers . The JobManager controls the execution of a single application. It receives an application for execution and builds a Task Execution Graph from the defined Job Graph. It manages job submission and the job lifecycle then allocates work to Task Managers. The Resource Manager manages Task Slots and leverages underlying orchestrator, like Kubernetes or Yarn. A Task slot is the unit of work executed on CPU. The Task Managers execute the actual stream processing logic. There are multiple task managers running in a cluster. The number of slots limits the number of tasks a TaskManager can execute. After it has been started, a TaskManager registers its slots to the ResourceManager To support the execution of Flink job, KDA provides resources using units called Kinesis Processing Units (KPUs). One KPU represents the following system resources: One CPU core 4 GB of memory, of which one GB is native memory and three GB are heap memory 50 GB of disk space The number of KPU = Parallelism parameter / ParallelismPerKPU parameter.","title":"Kinesis Data Analytics"},{"location":"design/#when-to-choose-what","text":"As Apache Flink is an open-source project, it is possible to deploy it in a Kubernetes cluster, using Flink operator. This will bring you with the most flexible solution as you can select the underlying EC2 instances needed, to optimize your cost. Also you will have fine-grained control over cluster settings, debugging tools and monitoring. While Kinesis Data Analytics helps you to focus on the application logic, which is not simple programming experience, as stateful processing is challenginf, there is no management of infrastructure, monitoring, auto scaling and high availability integrated in the service. In addition to the AWS integrations, the Kinesis Data Analytics libraries include more than 10 Apache Flink connectors and the ability to build custom integrations.","title":"When to choose what"},{"location":"design/#implementation-details","text":"See this dedicated note","title":"Implementation Details"},{"location":"design/#quicksight-integration-design","text":"We need to represent the following metrics within QuickSight: tenants, # of users job submitted over time per tenant last activity date per tenant number of job in last 30 and 90 days The Dashboard is supported by Amazon QuickSight, which helps us to develop Analysis from different datasources, with drill down capabilities. The datasources are in S3 bucket with files that are continuously updated by the Data Streaming and Analtics components. In QuiskSight data sources are mapped to data sets, and we can apply data transformation for a better usage in analysis and visualisation. Here is an example of User Interface constructs: For some implementation details see this note . In the future we can automate the deployment of this dashboard by using CloudFormation. See this note .","title":"QuickSight Integration Design"},{"location":"design/#api-gateway-and-lambda-function-for-sagemaker","text":"As introduced in figure 1, API Gateway is deployed to expose REST API, and specially one supported by a Lambda function in backend to proxy SageMaker. The goal here is to add an anti-corruption layer into the domain of data streaming which most of the time represent business events while SageMaker use csv type of input. There is a cloudFormation template defined in setup/cloudformation folder named APIGW-Lambda.yaml to declare API Gateway, role, and Lambda function. The following command create this stack in the connected Region: aws cloudformation create-stack --stack-name apigw-lambda-sm --template-body file://APIGW-Lambda.yaml --parameters ParameterKey = sagemakerEndpoint,ParameterValue = linear-learner-2022-12-22-23-12-40-646 --capabilities CAPABILITY_NAMED_IAM # Result looks like: { \"StackId\" : \"arn:aws:cloudformation:us-west-2:......:stack/apigw-lambda-sm/5041dce0-.....8258-11ed-8ddc-06ec61b22a8d\" } We can access the Lambda and perform a smoke test to verify we reach the SageMaker endpoint.","title":"API Gateway And Lambda Function for SageMaker"},{"location":"eks/","text":"Elastic Kubernetes Service \u00b6 Amazon EKS is a fully managed service to run Kubernetes. It is integrated with VPC for isolation, IAM for authentication, ELB for load distribution, and ECR for container image registry. Major characteristics \u00b6 Scale K8s control plane across multiple AZs. No need to install, operate and maintain k8s cluster. Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instance. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers or on to AWS Outposts . Fully compatible with other CNSF kubernetes","title":"EKS"},{"location":"eks/#elastic-kubernetes-service","text":"Amazon EKS is a fully managed service to run Kubernetes. It is integrated with VPC for isolation, IAM for authentication, ELB for load distribution, and ECR for container image registry.","title":"Elastic Kubernetes Service"},{"location":"eks/#major-characteristics","text":"Scale K8s control plane across multiple AZs. No need to install, operate and maintain k8s cluster. Automatically scales control plane instances based on load, detects and replaces unhealthy control plane instance. It supports EC2 to deploy worker nodes or Fargate to deploy serverless containers or on to AWS Outposts . Fully compatible with other CNSF kubernetes","title":"Major characteristics"},{"location":"java-apps/","text":"Java Based microservices \u00b6 This note address simple implementation and deployment detail of the two microservices used in the demonstration. Tenant Manager \u00b6 This is a Java Quarkus app, with Reactive REST API, persistence to RDS Postgresql via JPA and Panache. We recommend to review the following Quarkus guides to get started on this stack. GENERATING JAX-RS RESOURCES WITH PANACHE . Reactive messaging . Kubernetes extension . The service supports basic CRUD operation on the Tenant entity. Panache is doing the JPA mapping to JDBC. The OpenAPI extension is added to offer a swagger User interface so it easier to test the component. We use the Repository pattern for the Panache. The home page of the application from which we can access to the Swagger UI The Swagger UI with the defined OpenAPI The Stream processing in Flink application will code the GET /api/v1/tenants/{tenantID} API to get the information about the company doing one of the big data batch processing. Run locally \u00b6 Start docker compose for a local postgresql docker compose up -d Start quarkus in dev mode: quarkus dev if you have the CLI, or mvn dev with maven. Verify the service works locally with command like: # Get all tenants in the database curl -X GET localhost:8080/api/v1/tenants # Get one tenant curl -X GET localhost:8080/api/v1/tenants/comp_2 Build docker image and push it to ECR \u00b6 Create the ECR repository: named jbcodeforce/demo-saas-tenant-mgr ./scripts/createECRrepository.sh # OR to a new name ./scripts/createECRrepository.sh anothername/demo-saas-tenant-mgr The result may look like: { \"repository\" : { \"repositoryArn\" : \"arn:aws:ecr:us-west-2:v..accountnumber:repository/jbcodeforce/demo-saas-tenant-mgr\" , \"registryId\" : \"4..accountnumber\" , \"repositoryName\" : \"jbcodeforce/demo-saas-tenant-mgr\" , \"repositoryUri\" : \"4..accountnumber.dkr.ecr.us-west-2.amazonaws.com/jbcodeforce/demo-saas-tenant-mgr\" , \"createdAt\" : \"2022-12-28T09:48:16-08:00\" , \"imageTagMutability\" : \"MUTABLE\" , \"imageScanningConfiguration\" : { \"scanOnPush\" : false }, \"encryptionConfiguration\" : { \"encryptionType\" : \"AES256\" } } } Be sure to be logged to the ECR registry with a command like: aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 4 ..accountnumber.dkr.ecr.us-west-2.amazonaws.com Build and push the image using the ECR base URL for your region and account: ./scripts/buildAll.sh 40 ..accountnumber.dkr.ecr.us-west-2.amazonaws.com # verify docker images | grep demo-saas-tenant-mgr The reference documentation for ECR push image Deploy the app to EKS \u00b6 The following figure illustrates what we are deploying: One VPC with 2 Availability zones and one public and private subnet per AZ. EKS cluster defined in private subnet with 2 nodes One namespace to get the core of AnyCompany services (Control plane components) Amazon RDS to persist Tenant data RDS Postgresql database \u00b6 As a pre-requisite we need one instance of RDS postgresql database. It should have been created with the solution CDK, in setup/saas-solution-cdk . See CDK python doc Kubernetes extension for Quarkus \u00b6 We recommend to read the Kubernetes Quarkus guide . The Kubernetes extension was added to the pom.xml and a kubernetes.yml file is created at each build and includes deployment, services, service account YAML definitions. To tune this generated file the following declarations were added to the application.properties : quarkus.kubernetes.namespace = demo-saas-core quarkus.container-image.registry = 4....dkr.ecr.us-west-2.amazonaws.com/jbcodeforce/demo-saas-tenant-mgr quarkus.container-image.tag = latest quarkus.kubernetes.env.secrets = saas-secret Once EKS cluster is operational, using kubectl can deploy the application: kubectl apply -f target/kubernetes/kubernetes.yml","title":"Tenant Manager"},{"location":"java-apps/#java-based-microservices","text":"This note address simple implementation and deployment detail of the two microservices used in the demonstration.","title":"Java Based microservices"},{"location":"java-apps/#tenant-manager","text":"This is a Java Quarkus app, with Reactive REST API, persistence to RDS Postgresql via JPA and Panache. We recommend to review the following Quarkus guides to get started on this stack. GENERATING JAX-RS RESOURCES WITH PANACHE . Reactive messaging . Kubernetes extension . The service supports basic CRUD operation on the Tenant entity. Panache is doing the JPA mapping to JDBC. The OpenAPI extension is added to offer a swagger User interface so it easier to test the component. We use the Repository pattern for the Panache. The home page of the application from which we can access to the Swagger UI The Swagger UI with the defined OpenAPI The Stream processing in Flink application will code the GET /api/v1/tenants/{tenantID} API to get the information about the company doing one of the big data batch processing.","title":"Tenant Manager"},{"location":"java-apps/#run-locally","text":"Start docker compose for a local postgresql docker compose up -d Start quarkus in dev mode: quarkus dev if you have the CLI, or mvn dev with maven. Verify the service works locally with command like: # Get all tenants in the database curl -X GET localhost:8080/api/v1/tenants # Get one tenant curl -X GET localhost:8080/api/v1/tenants/comp_2","title":"Run locally"},{"location":"java-apps/#build-docker-image-and-push-it-to-ecr","text":"Create the ECR repository: named jbcodeforce/demo-saas-tenant-mgr ./scripts/createECRrepository.sh # OR to a new name ./scripts/createECRrepository.sh anothername/demo-saas-tenant-mgr The result may look like: { \"repository\" : { \"repositoryArn\" : \"arn:aws:ecr:us-west-2:v..accountnumber:repository/jbcodeforce/demo-saas-tenant-mgr\" , \"registryId\" : \"4..accountnumber\" , \"repositoryName\" : \"jbcodeforce/demo-saas-tenant-mgr\" , \"repositoryUri\" : \"4..accountnumber.dkr.ecr.us-west-2.amazonaws.com/jbcodeforce/demo-saas-tenant-mgr\" , \"createdAt\" : \"2022-12-28T09:48:16-08:00\" , \"imageTagMutability\" : \"MUTABLE\" , \"imageScanningConfiguration\" : { \"scanOnPush\" : false }, \"encryptionConfiguration\" : { \"encryptionType\" : \"AES256\" } } } Be sure to be logged to the ECR registry with a command like: aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 4 ..accountnumber.dkr.ecr.us-west-2.amazonaws.com Build and push the image using the ECR base URL for your region and account: ./scripts/buildAll.sh 40 ..accountnumber.dkr.ecr.us-west-2.amazonaws.com # verify docker images | grep demo-saas-tenant-mgr The reference documentation for ECR push image","title":"Build docker image and push it to ECR"},{"location":"java-apps/#deploy-the-app-to-eks","text":"The following figure illustrates what we are deploying: One VPC with 2 Availability zones and one public and private subnet per AZ. EKS cluster defined in private subnet with 2 nodes One namespace to get the core of AnyCompany services (Control plane components) Amazon RDS to persist Tenant data","title":"Deploy the app to EKS"},{"location":"java-apps/#rds-postgresql-database","text":"As a pre-requisite we need one instance of RDS postgresql database. It should have been created with the solution CDK, in setup/saas-solution-cdk . See CDK python doc","title":"RDS Postgresql database"},{"location":"java-apps/#kubernetes-extension-for-quarkus","text":"We recommend to read the Kubernetes Quarkus guide . The Kubernetes extension was added to the pom.xml and a kubernetes.yml file is created at each build and includes deployment, services, service account YAML definitions. To tune this generated file the following declarations were added to the application.properties : quarkus.kubernetes.namespace = demo-saas-core quarkus.container-image.registry = 4....dkr.ecr.us-west-2.amazonaws.com/jbcodeforce/demo-saas-tenant-mgr quarkus.container-image.tag = latest quarkus.kubernetes.env.secrets = saas-secret Once EKS cluster is operational, using kubectl can deploy the application: kubectl apply -f target/kubernetes/kubernetes.yml","title":"Kubernetes extension for Quarkus"},{"location":"model/","text":"Company Churn assess AI model development with SageMaker \u00b6 Introduction \u00b6 The goal of this service is to compute the risk for a customer to leave the SaaS platform. The scoring takes into account the industry type, the company size in term of revenue and number of employees, and then specifics features from the SaaS business model. In our case we will take a company doing big data job management so we can add variables: number of jobs in last 30 days, and 90 days, monthly charge, total charge, number of time tutorials were done so far cross all users. Here is simple example of training data: (Revenue is in million $) Company,Industry,Revenue,Employees,#job30,#job90,MonthlyCharge,TotalCharge,Churn comp_0,travel,9270,4635,3,56,446,1476,1 comp_1,finance,99420,49710,7,97,424,3039,0 comp_2,gov,83410,27803,7,66,1128,1422,0 comp_3,service,59650,19883,5,58,967,3661,0 comp_4,retail,29080,14540,4,13,461,1172,1 comp_5,finance,83590,20898,5,75,472,3735,0 comp_6,retail,86080,28693,9,85,824,4475,1 The following figure illustrates how to build the model using Amazon SageMaker (in green the components developed in this solution) using training and test data sets and then deployed model in a scalable runtime: Figure 1: SageMaker Model Training and Runtime deployment The runtime is exposing an Endpoint that we can access using ASW SDK from a lambda or a microservice , or Kinesis Data Analytics. To train the model we use a predefined algorithm, packaged as docker image, and available inside SageMaker notebook, via access to Amazon Elastic Container Registry. This repository includes a Simulator (CompanyDataGenerator.py) to generate data to build training set. The generated records can be uploaded to S3 via AWS CLI. It also includes a SageMaker notebook (company-churn-sm.ipynb) that can be executed in AWS SageMaker Studio to perform some simple feature engineering, train the model, and deploy the model to SageMaker runtime so it can be called from any clients which has the endpoint information. Preparing the data \u00b6 Within this repository under the CompanyRisk folder there is a simple Python program to be used to generate random data of companies within industry, with revenue, number of employees, # of jobs submitted the last 30 days, 90 days, current monthly fees and accumulated fees. The companies.csv file was already created from a previous run and can be used as source for training. Run the simulator \u00b6 If you want to re-run the simulator you can do the following steps. Be sure to have last AWS CLI and python library. You can use AWS Powershell and clone this repository, or if you use your own computer, you can use a docker image built from a dockerfile in aws-studies labs folder : docker build -f https://raw.githubusercontent.com/jbcodeforce/aws-studies/main/labs/Dockerfile -t jbcodeforce/aws-python . Start the python environment with docker, mounting the source code to /app : docker run --rm --name pythonapp -v $( pwd ) :/app -v ~/.aws:/root/.aws -it -p 5000 :5000 jbcodeforce/aws-python bash Run the data generator: python CompanyDataGenerator.py companies.csv --nb_records 10000 This should create companies.csv file with 10,000 rows Simulator code explanations \u00b6 Use argparser to define the argument for the command line. Generate nb_records row: company has unique id, industry is selected randomly, revenue and number of employee is linked to the revenue. Churn flag is set to 1 if revenue is low. Use csv library to write the csv file. Upload generated files to S3 \u00b6 Pre-requisites \u00b6 Be sure to have an IAM role with S3FullAccess. The name of the role is () Get Access Key and Secret key and configuge aws, we specific profile: aws configure --profile s3admin Verify you can access s3 using: aws s3 ls . Use the command: aws s3 cp $PWD /companies.csv s3://jb-data-set/churn/companies.csv --profile s3admin [Alternate] Start the Python 3 environment using docker docker run --rm --name pythonapp -v $( pwd ) :/app -v ~/.aws:/root/.aws -it -p 5000 :5000 jbcodeforce/aws-python bash Using the python code and boto3 library do the following: python copyToS3.py us-west-2 jb-data-set churn $PWD /companies.csv If some libraries are not installed do pip install -r requirements.txt Build and deploy the model with AWS SageMaker \u00b6 The goal of this section is to build the churn predictive scoring model within SageMaker. The steps are simple: Create or use your SageMaker Studio, here are workshop instructions to do so. Be sure to have an IAM role for SageMaker to access remote services like S3. In the folder manager create a new folder named SaaS-company-churn and then upload the companies.csv file and the SageMaker Notebook company-churn-sm.ipynb Executes the steps one by one or all of them, it should create the training set, test sets, build and deploy the model using the SageMaker Python API. Be sure to keep a copy of the name of the service endpoint as it is needed for the client to call this service in future steps. Deeper dive: build model \u00b6 The notebook work is quite simple: remove unnecessary columns, transform categorical features to one hot columns. Split 20% of the records to build a test set, the rest being the training set. industryCat = df [ \"Industry\" ] . unique () industry_type = CategoricalDtype ( categories = industryCat ) df = df . drop ([ 'Company' ], axis = 1 ) df = pd . get_dummies ( df , columns = [ 'Industry' ]) # SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column. cols = df . columns . tolist () cols = [ cols [ 6 ]] + cols [: 5 ] + cols [ 7 :] df = df [ cols ] train_data , test_data , _ = np . split ( df . sample ( frac = 1 , random_state = 1729 ), [ int ( 0.8 * len ( df )), len ( df )]) train_data . to_csv ( 'train.csv' , header = False , index = False ) test_data . to_csv ( 'test.csv' , header = False , index = False ) Using Python AWS SDK named boto3, we can upload the created data sets to S3 bucket boto3 . Session () . resource ( 's3' ) . Bucket ( data_bucket ) . Object ( os . path . join ( PREFIX , 'train/train.csv' )) . upload_file ( 'train.csv' ) boto3 . Session () . resource ( 's3' ) . Bucket ( data_bucket ) . Object ( os . path . join ( PREFIX , 'validation/test.csv' )) . upload_file ( 'test.csv' ) and then use sagemaker SDK to define input and validation sets: train_data = sagemaker . inputs . TrainingInput ( s3_train_data , distribution = \"FullyReplicated\" , content_type = \"text/csv\" , s3_data_type = \"S3Prefix\" , record_wrapping = None , compression = None , ) validation_data = sagemaker . inputs . TrainingInput ( s3_validation_data , distribution = \"FullyReplicated\" , content_type = \"text/csv\" , s3_data_type = \"S3Prefix\" , record_wrapping = None , compression = None , ) As illustrated in figure above, we are using pre-build algorithm in the form of SageMaker container image: from sagemaker.image_uris import retrieve container = retrieve ( \"linear-learner\" , boto3 . Session () . region_name , version = \"1\" ) print ( container ) And then fit the model by using an LinearLearner estimator as a supervised learning algorithms used for solving classification or regression problems. from time import gmtime , strftime job_name = \"Linear-learner-company-churn-\" + strftime ( \"%H-%M-%S\" , gmtime ()) print ( \"Training job\" , job_name ) linear = sagemaker . estimator . Estimator ( container , role , input_mode = \"File\" , instance_count = 1 , instance_type = \"ml.m4.xlarge\" , output_path = output_location , sagemaker_session = sess , ) linear . set_hyperparameters ( epochs = 16 , wd = 0.01 , loss = \"absolute_loss\" , predictor_type = \"binary_classifier\" , normalize_data = True , optimizer = \"adam\" , mini_batch_size = 1000 , lr_scheduler_step = 100 , lr_scheduler_factor = 0.99 , lr_scheduler_minimum_lr = 0.0001 , learning_rate = 0.1 , ) linear . fit ( inputs = { \"train\" : train_data , \"validation\" : validation_data }, job_name = job_name ) Deploy the model with the following code: linear_predictor = linear . deploy ( initial_instance_count = 1 , instance_type = \"ml.c4.xlarge\" ) print ( f \" \\n created endpoint: { linear_predictor . endpoint_name } \" ) Be sure to keep the name of the endpoint , as we will need it for the lambda . We can try some prediction in the notebook using cless like: payload = \"19100,9550,6,39,227,810,0,0,0,0,0,0,1,0\" result = linear_predictor . predict ( payload ) print ( result ) # Result looks like: { 'predictions' : [{ 'score' : 0.985599935054779 , 'predicted_label' : 1 }]} Or using a python client, you can run inside a EC2 or on your computer see code CompanyRisk/CallSageMakerRunTime.py , be sure to set the ENDPOINT variable to the SageMaker endpoint name. The URL is not public, but could also being accessed via an HTTP POST from an EC2 in the same VPC. Read more \u00b6 SageMaker - Linear Learner SageMaker - Common Data Formats for Training","title":"ML model"},{"location":"model/#company-churn-assess-ai-model-development-with-sagemaker","text":"","title":"Company Churn assess AI model development with SageMaker"},{"location":"model/#introduction","text":"The goal of this service is to compute the risk for a customer to leave the SaaS platform. The scoring takes into account the industry type, the company size in term of revenue and number of employees, and then specifics features from the SaaS business model. In our case we will take a company doing big data job management so we can add variables: number of jobs in last 30 days, and 90 days, monthly charge, total charge, number of time tutorials were done so far cross all users. Here is simple example of training data: (Revenue is in million $) Company,Industry,Revenue,Employees,#job30,#job90,MonthlyCharge,TotalCharge,Churn comp_0,travel,9270,4635,3,56,446,1476,1 comp_1,finance,99420,49710,7,97,424,3039,0 comp_2,gov,83410,27803,7,66,1128,1422,0 comp_3,service,59650,19883,5,58,967,3661,0 comp_4,retail,29080,14540,4,13,461,1172,1 comp_5,finance,83590,20898,5,75,472,3735,0 comp_6,retail,86080,28693,9,85,824,4475,1 The following figure illustrates how to build the model using Amazon SageMaker (in green the components developed in this solution) using training and test data sets and then deployed model in a scalable runtime: Figure 1: SageMaker Model Training and Runtime deployment The runtime is exposing an Endpoint that we can access using ASW SDK from a lambda or a microservice , or Kinesis Data Analytics. To train the model we use a predefined algorithm, packaged as docker image, and available inside SageMaker notebook, via access to Amazon Elastic Container Registry. This repository includes a Simulator (CompanyDataGenerator.py) to generate data to build training set. The generated records can be uploaded to S3 via AWS CLI. It also includes a SageMaker notebook (company-churn-sm.ipynb) that can be executed in AWS SageMaker Studio to perform some simple feature engineering, train the model, and deploy the model to SageMaker runtime so it can be called from any clients which has the endpoint information.","title":"Introduction"},{"location":"model/#preparing-the-data","text":"Within this repository under the CompanyRisk folder there is a simple Python program to be used to generate random data of companies within industry, with revenue, number of employees, # of jobs submitted the last 30 days, 90 days, current monthly fees and accumulated fees. The companies.csv file was already created from a previous run and can be used as source for training.","title":"Preparing the data"},{"location":"model/#run-the-simulator","text":"If you want to re-run the simulator you can do the following steps. Be sure to have last AWS CLI and python library. You can use AWS Powershell and clone this repository, or if you use your own computer, you can use a docker image built from a dockerfile in aws-studies labs folder : docker build -f https://raw.githubusercontent.com/jbcodeforce/aws-studies/main/labs/Dockerfile -t jbcodeforce/aws-python . Start the python environment with docker, mounting the source code to /app : docker run --rm --name pythonapp -v $( pwd ) :/app -v ~/.aws:/root/.aws -it -p 5000 :5000 jbcodeforce/aws-python bash Run the data generator: python CompanyDataGenerator.py companies.csv --nb_records 10000 This should create companies.csv file with 10,000 rows","title":"Run the simulator"},{"location":"model/#simulator-code-explanations","text":"Use argparser to define the argument for the command line. Generate nb_records row: company has unique id, industry is selected randomly, revenue and number of employee is linked to the revenue. Churn flag is set to 1 if revenue is low. Use csv library to write the csv file.","title":"Simulator code explanations"},{"location":"model/#upload-generated-files-to-s3","text":"","title":"Upload generated files to S3"},{"location":"model/#pre-requisites","text":"Be sure to have an IAM role with S3FullAccess. The name of the role is () Get Access Key and Secret key and configuge aws, we specific profile: aws configure --profile s3admin Verify you can access s3 using: aws s3 ls . Use the command: aws s3 cp $PWD /companies.csv s3://jb-data-set/churn/companies.csv --profile s3admin [Alternate] Start the Python 3 environment using docker docker run --rm --name pythonapp -v $( pwd ) :/app -v ~/.aws:/root/.aws -it -p 5000 :5000 jbcodeforce/aws-python bash Using the python code and boto3 library do the following: python copyToS3.py us-west-2 jb-data-set churn $PWD /companies.csv If some libraries are not installed do pip install -r requirements.txt","title":"Pre-requisites"},{"location":"model/#build-and-deploy-the-model-with-aws-sagemaker","text":"The goal of this section is to build the churn predictive scoring model within SageMaker. The steps are simple: Create or use your SageMaker Studio, here are workshop instructions to do so. Be sure to have an IAM role for SageMaker to access remote services like S3. In the folder manager create a new folder named SaaS-company-churn and then upload the companies.csv file and the SageMaker Notebook company-churn-sm.ipynb Executes the steps one by one or all of them, it should create the training set, test sets, build and deploy the model using the SageMaker Python API. Be sure to keep a copy of the name of the service endpoint as it is needed for the client to call this service in future steps.","title":"Build and deploy the model with AWS SageMaker"},{"location":"model/#deeper-dive-build-model","text":"The notebook work is quite simple: remove unnecessary columns, transform categorical features to one hot columns. Split 20% of the records to build a test set, the rest being the training set. industryCat = df [ \"Industry\" ] . unique () industry_type = CategoricalDtype ( categories = industryCat ) df = df . drop ([ 'Company' ], axis = 1 ) df = pd . get_dummies ( df , columns = [ 'Industry' ]) # SageMaker requires that a CSV file does not have a header record and that the target variable is in the first column. cols = df . columns . tolist () cols = [ cols [ 6 ]] + cols [: 5 ] + cols [ 7 :] df = df [ cols ] train_data , test_data , _ = np . split ( df . sample ( frac = 1 , random_state = 1729 ), [ int ( 0.8 * len ( df )), len ( df )]) train_data . to_csv ( 'train.csv' , header = False , index = False ) test_data . to_csv ( 'test.csv' , header = False , index = False ) Using Python AWS SDK named boto3, we can upload the created data sets to S3 bucket boto3 . Session () . resource ( 's3' ) . Bucket ( data_bucket ) . Object ( os . path . join ( PREFIX , 'train/train.csv' )) . upload_file ( 'train.csv' ) boto3 . Session () . resource ( 's3' ) . Bucket ( data_bucket ) . Object ( os . path . join ( PREFIX , 'validation/test.csv' )) . upload_file ( 'test.csv' ) and then use sagemaker SDK to define input and validation sets: train_data = sagemaker . inputs . TrainingInput ( s3_train_data , distribution = \"FullyReplicated\" , content_type = \"text/csv\" , s3_data_type = \"S3Prefix\" , record_wrapping = None , compression = None , ) validation_data = sagemaker . inputs . TrainingInput ( s3_validation_data , distribution = \"FullyReplicated\" , content_type = \"text/csv\" , s3_data_type = \"S3Prefix\" , record_wrapping = None , compression = None , ) As illustrated in figure above, we are using pre-build algorithm in the form of SageMaker container image: from sagemaker.image_uris import retrieve container = retrieve ( \"linear-learner\" , boto3 . Session () . region_name , version = \"1\" ) print ( container ) And then fit the model by using an LinearLearner estimator as a supervised learning algorithms used for solving classification or regression problems. from time import gmtime , strftime job_name = \"Linear-learner-company-churn-\" + strftime ( \"%H-%M-%S\" , gmtime ()) print ( \"Training job\" , job_name ) linear = sagemaker . estimator . Estimator ( container , role , input_mode = \"File\" , instance_count = 1 , instance_type = \"ml.m4.xlarge\" , output_path = output_location , sagemaker_session = sess , ) linear . set_hyperparameters ( epochs = 16 , wd = 0.01 , loss = \"absolute_loss\" , predictor_type = \"binary_classifier\" , normalize_data = True , optimizer = \"adam\" , mini_batch_size = 1000 , lr_scheduler_step = 100 , lr_scheduler_factor = 0.99 , lr_scheduler_minimum_lr = 0.0001 , learning_rate = 0.1 , ) linear . fit ( inputs = { \"train\" : train_data , \"validation\" : validation_data }, job_name = job_name ) Deploy the model with the following code: linear_predictor = linear . deploy ( initial_instance_count = 1 , instance_type = \"ml.c4.xlarge\" ) print ( f \" \\n created endpoint: { linear_predictor . endpoint_name } \" ) Be sure to keep the name of the endpoint , as we will need it for the lambda . We can try some prediction in the notebook using cless like: payload = \"19100,9550,6,39,227,810,0,0,0,0,0,0,1,0\" result = linear_predictor . predict ( payload ) print ( result ) # Result looks like: { 'predictions' : [{ 'score' : 0.985599935054779 , 'predicted_label' : 1 }]} Or using a python client, you can run inside a EC2 or on your computer see code CompanyRisk/CallSageMakerRunTime.py , be sure to set the ENDPOINT variable to the SageMaker endpoint name. The URL is not public, but could also being accessed via an HTTP POST from an EC2 in the same VPC.","title":"Deeper dive: build model"},{"location":"model/#read-more","text":"SageMaker - Linear Learner SageMaker - Common Data Formats for Training","title":"Read more"},{"location":"rt-analytics/","text":"Real-time analytics with Kinesis Data Analytics \u00b6 The goal of this component is to compute stateful analytics, do data transformation, from the data coming in streams. The current implementation illustrates remote async calls to SageMaker (via API Gateway) and persistence to S3. Figure 1: Streaming components Kinesis Data Streams \u00b6 There is nothing special in this demonstration, the creation of the different data streams is done using CDK. bigdatajobs : for job related events companies : for event related to tenant entities events enrichedcompanies to shared events with enriched data to be consumed by other services. See the python file kinesis-cdk/app.py for the CDK definitions of those streams and to deploy them, do a cdk deploy under the folder: setup/kinesis-cdk . The persistence is set to 24 hours. Kinesis Data Analytics \u00b6 Why Kinesis Data Analytics \u00b6 This is a managed service, serverless, to run real-time streaming logic implemented using SQL, java, Scala. The core technology used is Apache Flink. The service is integrated with Kinesis Data Streams, and with a lot of AWS services to serve as sources or sinks. Kinesis Data Analytics provides the underlying infrastructure for Flink applications. It handles core capabilities like provisioning compute resources, parallel computation, automatic scaling, and application backups implemented as checkpoints and snapshots. Code base \u00b6 The code is under rt-analytics/bg-job-processing folder. The main input stream includes the job events, and are published to Kinesis Data Streams names bigdatajobs . Once received we need to enrich with the company data, which lead to an asynchronous call to TenantManager service. The Company response includes: company_id, industry, revenu, employees, job30, job90, monthlyFee, totalFee in a form of JSON document. Once Company data is collected, the call to the ML scoring model is also done asynchronously, the churn flag may be update. The final step is to write to S3 bucket. Parameters \u00b6 The following parameters need to be specified Parameter Description Default aws.region Region where all the services run us-west-2 jobs.stream.initial.position Position in the streams to start consuming from LATEST jobs.stream.name Job events stream name bigdatajobs S3SinkPath Bucket to write enriched company events predictChurnApiEndpoint URL of the API in API gateway predictChurnApiKey None for API Gateway Those parameters are defined in the Application Creation request. See create-request.json file EnvironmentProperties in request.json json \"EnvironmentProperties\": { \"PropertyGroups\": [ { \"PropertyGroupId\": \"ProducerConfigProperties\", \"PropertyMap\": { \"flink.stream.initpos\": \"LATEST\", \"aws.region\": \"us-west-2\", \"AggregationEnabled\": \"false\" } }, { \"PropertyGroupId\": \"ApplicationConfigProperties\", \"PropertyMap\": { \"predictChurnApiEndpoint\": \"https://API.execute-api.us-west-2.amazonaws.com/prod/assessChurn\", \"predictChurnApiKey\" : \" \", \"S3SinkPath\": \"s3://jb-data-set/churn/data\" } }, { \"PropertyGroupId\": \"ConsumerConfigProperties\", \"PropertyMap\": { \"aws.region\": \"us-west-2\", \"jobs.stream.initial.position\": \"LATEST\", \"jobs.stream.name\": \"bigdatajobs\", \"companies.stream.initial.position\": \"LATEST\", \"companies.stream.name\": \"companies\" } } ] }, Code approach \u00b6 The Flink data source is a KinesisConsumer and the declaration looks mostly always the same: import org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer ; // in the main() final StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); env . addSource ( new FlinkKinesisConsumer <> ( inputStreamName , new SimpleStringSchema (), inputProperties )); Then we want to map the job information and extract the company_ID and map to a Company entity: DataStream < Company > companyStream = jobStreams . map ( jobEvent -> { String [] words = jobEvent . split ( \",\" ); Company c = getCompanyRecord ( words [ 0 ] ); return c ; }); The asynchronous call to the API Gateway and then sageMaker is done using Flink construct and custom class for HttpRequest: DataStream < HttpRequest < Company >> predictChurnRequest = companyStream . map ( company -> { return new HttpRequest < Company > ( company , SdkHttpMethod . POST ). withBody ( mapper . writeValueAsString ( company )); }) . returns ( new TypeHint < HttpRequest < Company >> () {});; DataStream < HttpResponse < Company >> predictChurnResponse = // Asynchronously call Endpoint AsyncDataStream . unorderedWait ( predictChurnRequest , new Sig4SignedHttpRequestAsyncFunction <> ( predictChurnEndpoint ), 30 , TimeUnit . SECONDS , 20 ) . returns ( new TypeHint < HttpResponse < Company >> () {}); DataStream < Company > enrichedCompany = predictChurnResponse // Only keep successful responses for enrichment, which have a 200 status code . filter ( response -> response . statusCode == 200 ) // Enrich Company with response from predictChurn . map ( response -> { boolean expectedChurn = mapper . readValue ( response . responseBody , ObjectNode . class ). get ( \"churn\" ). asBoolean (); return response . triggeringEvent . withExpectedChurn ( expectedChurn ); }); Here is an example of the job definition as deployed on AWS Data Analytics: Deeper dive \u00b6 Real Time ML inference on Streaming Data Streaming Data Solution for Amazon Kinesis Git repo S3 sink with Flink Deployment \u00b6 Create IAM role to support application execution We need to create a role and permission policy so the application can access source and sink resources and assume the role for kinesisanalytics.amazonaws.com service: aws iam create-role --role-name CompanyAnalyticsRole --assume-role-policy-document file://trust-relationship.json We need to define permissions policy with two statements: one that grants permissions for the read action on the source streams, and another that grants permissions for write actions on the sink stream which will be S3 bucker and Data Streams: IAM policy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"ReadCodeFromS3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" , \"s3:GetObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::jb-data-set/churn/bg-job-processing-1.0.0.jar\" ] }, { \"Sid\" : \"CompanySinkS3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"logs:DescribeLogStreams\" ], \"Resource\" : [ \"arn:aws:s3:::jb-data-set/churn/*\" , \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:*\" ] }, { \"Sid\" : \"DescribeLogGroups\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogGroups\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:*\" ] }, { \"Sid\" : \"DescribeLogStreams\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:*\" ] }, { \"Sid\" : \"PutCloudwatchLogs\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:PutLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:kinesis-analytics-log-stream\" ] }, { \"Sid\" : \"ReadInputStream\" , \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : [ \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/companies\" , \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/bigdatajobs\" ] }, { \"Sid\" : \"WriteOutputStream\" , \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/enrichedcompanies\" } ] } Attach the policy to an IAM role aws iam put-role-policy --role-name CompanyAnalyticsRole --policy-name KinesisPolicy --policy-document file://security-policy.json Build the java packaging and upload it to S3 # build the uber jar mvn package -Dflink.version = 1 .15.2 # upload to S3 aws s3 cp $( pwd ) /target/bg-job-processing-1.0.0.jar s3://jb-data-set/churn/bg-job-processing-1.0.0.jar Create the Data Analytics application sh aws kinesisanalyticsv2 create-application --cli-input-json file://create_request.json Start the Kinesis Analytics apps aws kinesisanalyticsv2 start-application --cli-input-json file://start_application.json Send some data as job manager will do (use Start Python Env docker container): python src/main/python/SendJobEventToKinesis.py To update code, we need to delete previous code, upload new version to S3, and update application: aws s3 rm s3://jb-data-set/churn/bg-job-processing-1.0.0.jar aws s3 cp $( pwd ) /target/bg-job-processing-1.0.0.jar s3://jb-data-set/churn/bg-job-processing-1.0.0.jar # Get application ID aws kinesisanalyticsv2 describe-application --application-name CompanyJobProcessing # result looks like { \"ApplicationDetail\" : { \"ApplicationARN\" : \"arn:aws:kinesisanalytics:us-west-2:4....6:application/CompanyJobProcessing\" , \"ApplicationDescription\" : \"Java Flink app to merge company and big data job events\" , \"ApplicationName\" : \"CompanyJobProcessing\" , \"RuntimeEnvironment\" : \"FLINK-1_15\" , \"ServiceExecutionRole\" : \"arn:aws:iam::....:role/CompanyAnalyticsRole\" , \"ApplicationStatus\" : \"RUNNING\" , \"ApplicationVersionId\" : 3 , } # Modify the updateApplication.json file with the application ID aws kinesisanalyticsv2 update-application --application-name CompanyJobProcessing --cli-input-json file://updateApplication.json Manual deployment with the Console \u00b6 Using the Kinesis console we can add an Analytics Application: Select the Flink runtime version: Select the IAM role or create a new one. For demonstration we can use the Development deployment with 1 Add configuration detail to get packaged code: `` Specifying the logging level Once deployed, start the job with the Run without snapshot option: Clean up \u00b6 aws kinesisanalyticsv2 stop-application --application-name CompanyJobProcessing --force aws kinesisanalyticsv2 describe-application --application-name CompanyJobProcessing | jq aws kinesisanalyticsv2 delete-application --application-name CompanyJobProcessing --create-timestamp 2022 -12-23T17:02:09-08:00 aws s3 rm s3://jb-data-set/churn/bg-job-processing-1.0.0.jar Logging in CloudWatch \u00b6 As logging as enabled in the configuration, we can see the details of the job execution in the Log groups named /aws/kinesis-analytics/CompanyJobProcessing to rework \u00b6 Joins between company and job streams on the company ID and add the number of jobs run (from job event) to the company current jobs count. Job is: company_id, userid , #job_submitted Out come is : company_id, industry, revenu, employees, job30 + #job_submitted, job90 + #job_submitted, monthlyFee, totalFee Current error \u00b6 2023 - 01 - 04 21 : 43 : 34 com . esotericsoftware . kryo . KryoException : Unable to find class : software . amazon . awssdk . internal . http . LowCopyListMap$$Lambda$1273 / 0x0000000800edc840 Serialization trace : mapConstructor ( software . amazon . awssdk . internal . http . LowCopyListMap$ForBuilder ) headers ( software . amazon . awssdk . http . DefaultSdkHttpFullRequest$Builder ) builder ( jbcodeforce . bgdatajob . operators . Sig4SignedHttpRequestAsyncFunction$HttpRequest ) at com . esotericsoftware . kryo . util . DefaultClassResolver . readName ( DefaultClassResolver . java : 138 ) at com . esotericsoftware . kryo . util . DefaultClassResolver . readClass ( DefaultClassResolver . java : 115 )","title":"RT Analytics"},{"location":"rt-analytics/#real-time-analytics-with-kinesis-data-analytics","text":"The goal of this component is to compute stateful analytics, do data transformation, from the data coming in streams. The current implementation illustrates remote async calls to SageMaker (via API Gateway) and persistence to S3. Figure 1: Streaming components","title":"Real-time analytics with Kinesis Data Analytics"},{"location":"rt-analytics/#kinesis-data-streams","text":"There is nothing special in this demonstration, the creation of the different data streams is done using CDK. bigdatajobs : for job related events companies : for event related to tenant entities events enrichedcompanies to shared events with enriched data to be consumed by other services. See the python file kinesis-cdk/app.py for the CDK definitions of those streams and to deploy them, do a cdk deploy under the folder: setup/kinesis-cdk . The persistence is set to 24 hours.","title":"Kinesis Data Streams"},{"location":"rt-analytics/#kinesis-data-analytics","text":"","title":"Kinesis Data Analytics"},{"location":"rt-analytics/#why-kinesis-data-analytics","text":"This is a managed service, serverless, to run real-time streaming logic implemented using SQL, java, Scala. The core technology used is Apache Flink. The service is integrated with Kinesis Data Streams, and with a lot of AWS services to serve as sources or sinks. Kinesis Data Analytics provides the underlying infrastructure for Flink applications. It handles core capabilities like provisioning compute resources, parallel computation, automatic scaling, and application backups implemented as checkpoints and snapshots.","title":"Why Kinesis Data Analytics"},{"location":"rt-analytics/#code-base","text":"The code is under rt-analytics/bg-job-processing folder. The main input stream includes the job events, and are published to Kinesis Data Streams names bigdatajobs . Once received we need to enrich with the company data, which lead to an asynchronous call to TenantManager service. The Company response includes: company_id, industry, revenu, employees, job30, job90, monthlyFee, totalFee in a form of JSON document. Once Company data is collected, the call to the ML scoring model is also done asynchronously, the churn flag may be update. The final step is to write to S3 bucket.","title":"Code base"},{"location":"rt-analytics/#parameters","text":"The following parameters need to be specified Parameter Description Default aws.region Region where all the services run us-west-2 jobs.stream.initial.position Position in the streams to start consuming from LATEST jobs.stream.name Job events stream name bigdatajobs S3SinkPath Bucket to write enriched company events predictChurnApiEndpoint URL of the API in API gateway predictChurnApiKey None for API Gateway Those parameters are defined in the Application Creation request. See create-request.json file EnvironmentProperties in request.json json \"EnvironmentProperties\": { \"PropertyGroups\": [ { \"PropertyGroupId\": \"ProducerConfigProperties\", \"PropertyMap\": { \"flink.stream.initpos\": \"LATEST\", \"aws.region\": \"us-west-2\", \"AggregationEnabled\": \"false\" } }, { \"PropertyGroupId\": \"ApplicationConfigProperties\", \"PropertyMap\": { \"predictChurnApiEndpoint\": \"https://API.execute-api.us-west-2.amazonaws.com/prod/assessChurn\", \"predictChurnApiKey\" : \" \", \"S3SinkPath\": \"s3://jb-data-set/churn/data\" } }, { \"PropertyGroupId\": \"ConsumerConfigProperties\", \"PropertyMap\": { \"aws.region\": \"us-west-2\", \"jobs.stream.initial.position\": \"LATEST\", \"jobs.stream.name\": \"bigdatajobs\", \"companies.stream.initial.position\": \"LATEST\", \"companies.stream.name\": \"companies\" } } ] },","title":"Parameters"},{"location":"rt-analytics/#code-approach","text":"The Flink data source is a KinesisConsumer and the declaration looks mostly always the same: import org.apache.flink.streaming.connectors.kinesis.FlinkKinesisConsumer ; // in the main() final StreamExecutionEnvironment env = StreamExecutionEnvironment . getExecutionEnvironment (); env . addSource ( new FlinkKinesisConsumer <> ( inputStreamName , new SimpleStringSchema (), inputProperties )); Then we want to map the job information and extract the company_ID and map to a Company entity: DataStream < Company > companyStream = jobStreams . map ( jobEvent -> { String [] words = jobEvent . split ( \",\" ); Company c = getCompanyRecord ( words [ 0 ] ); return c ; }); The asynchronous call to the API Gateway and then sageMaker is done using Flink construct and custom class for HttpRequest: DataStream < HttpRequest < Company >> predictChurnRequest = companyStream . map ( company -> { return new HttpRequest < Company > ( company , SdkHttpMethod . POST ). withBody ( mapper . writeValueAsString ( company )); }) . returns ( new TypeHint < HttpRequest < Company >> () {});; DataStream < HttpResponse < Company >> predictChurnResponse = // Asynchronously call Endpoint AsyncDataStream . unorderedWait ( predictChurnRequest , new Sig4SignedHttpRequestAsyncFunction <> ( predictChurnEndpoint ), 30 , TimeUnit . SECONDS , 20 ) . returns ( new TypeHint < HttpResponse < Company >> () {}); DataStream < Company > enrichedCompany = predictChurnResponse // Only keep successful responses for enrichment, which have a 200 status code . filter ( response -> response . statusCode == 200 ) // Enrich Company with response from predictChurn . map ( response -> { boolean expectedChurn = mapper . readValue ( response . responseBody , ObjectNode . class ). get ( \"churn\" ). asBoolean (); return response . triggeringEvent . withExpectedChurn ( expectedChurn ); }); Here is an example of the job definition as deployed on AWS Data Analytics:","title":"Code approach"},{"location":"rt-analytics/#deeper-dive","text":"Real Time ML inference on Streaming Data Streaming Data Solution for Amazon Kinesis Git repo S3 sink with Flink","title":"Deeper dive"},{"location":"rt-analytics/#deployment","text":"Create IAM role to support application execution We need to create a role and permission policy so the application can access source and sink resources and assume the role for kinesisanalytics.amazonaws.com service: aws iam create-role --role-name CompanyAnalyticsRole --assume-role-policy-document file://trust-relationship.json We need to define permissions policy with two statements: one that grants permissions for the read action on the source streams, and another that grants permissions for write actions on the sink stream which will be S3 bucker and Data Streams: IAM policy { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"ReadCodeFromS3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:GetObject\" , \"s3:GetObjectVersion\" ], \"Resource\" : [ \"arn:aws:s3:::jb-data-set/churn/bg-job-processing-1.0.0.jar\" ] }, { \"Sid\" : \"CompanySinkS3\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"s3:PutObject\" , \"logs:DescribeLogStreams\" ], \"Resource\" : [ \"arn:aws:s3:::jb-data-set/churn/*\" , \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:*\" ] }, { \"Sid\" : \"DescribeLogGroups\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogGroups\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:*\" ] }, { \"Sid\" : \"DescribeLogStreams\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:DescribeLogStreams\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:*\" ] }, { \"Sid\" : \"PutCloudwatchLogs\" , \"Effect\" : \"Allow\" , \"Action\" : [ \"logs:PutLogEvents\" ], \"Resource\" : [ \"arn:aws:logs:us-west-2:ACCOUNT_NUMBER:log-group:/aws/kinesis-analytics/CompanyJobProcessing:log-stream:kinesis-analytics-log-stream\" ] }, { \"Sid\" : \"ReadInputStream\" , \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : [ \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/companies\" , \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/bigdatajobs\" ] }, { \"Sid\" : \"WriteOutputStream\" , \"Effect\" : \"Allow\" , \"Action\" : \"kinesis:*\" , \"Resource\" : \"arn:aws:kinesis:us-west-2:ACCOUNT_NUMBER:stream/enrichedcompanies\" } ] } Attach the policy to an IAM role aws iam put-role-policy --role-name CompanyAnalyticsRole --policy-name KinesisPolicy --policy-document file://security-policy.json Build the java packaging and upload it to S3 # build the uber jar mvn package -Dflink.version = 1 .15.2 # upload to S3 aws s3 cp $( pwd ) /target/bg-job-processing-1.0.0.jar s3://jb-data-set/churn/bg-job-processing-1.0.0.jar Create the Data Analytics application sh aws kinesisanalyticsv2 create-application --cli-input-json file://create_request.json Start the Kinesis Analytics apps aws kinesisanalyticsv2 start-application --cli-input-json file://start_application.json Send some data as job manager will do (use Start Python Env docker container): python src/main/python/SendJobEventToKinesis.py To update code, we need to delete previous code, upload new version to S3, and update application: aws s3 rm s3://jb-data-set/churn/bg-job-processing-1.0.0.jar aws s3 cp $( pwd ) /target/bg-job-processing-1.0.0.jar s3://jb-data-set/churn/bg-job-processing-1.0.0.jar # Get application ID aws kinesisanalyticsv2 describe-application --application-name CompanyJobProcessing # result looks like { \"ApplicationDetail\" : { \"ApplicationARN\" : \"arn:aws:kinesisanalytics:us-west-2:4....6:application/CompanyJobProcessing\" , \"ApplicationDescription\" : \"Java Flink app to merge company and big data job events\" , \"ApplicationName\" : \"CompanyJobProcessing\" , \"RuntimeEnvironment\" : \"FLINK-1_15\" , \"ServiceExecutionRole\" : \"arn:aws:iam::....:role/CompanyAnalyticsRole\" , \"ApplicationStatus\" : \"RUNNING\" , \"ApplicationVersionId\" : 3 , } # Modify the updateApplication.json file with the application ID aws kinesisanalyticsv2 update-application --application-name CompanyJobProcessing --cli-input-json file://updateApplication.json","title":"Deployment"},{"location":"rt-analytics/#manual-deployment-with-the-console","text":"Using the Kinesis console we can add an Analytics Application: Select the Flink runtime version: Select the IAM role or create a new one. For demonstration we can use the Development deployment with 1 Add configuration detail to get packaged code: `` Specifying the logging level Once deployed, start the job with the Run without snapshot option:","title":"Manual deployment with the Console"},{"location":"rt-analytics/#clean-up","text":"aws kinesisanalyticsv2 stop-application --application-name CompanyJobProcessing --force aws kinesisanalyticsv2 describe-application --application-name CompanyJobProcessing | jq aws kinesisanalyticsv2 delete-application --application-name CompanyJobProcessing --create-timestamp 2022 -12-23T17:02:09-08:00 aws s3 rm s3://jb-data-set/churn/bg-job-processing-1.0.0.jar","title":"Clean up"},{"location":"rt-analytics/#logging-in-cloudwatch","text":"As logging as enabled in the configuration, we can see the details of the job execution in the Log groups named /aws/kinesis-analytics/CompanyJobProcessing","title":"Logging in CloudWatch"},{"location":"rt-analytics/#to-rework","text":"Joins between company and job streams on the company ID and add the number of jobs run (from job event) to the company current jobs count. Job is: company_id, userid , #job_submitted Out come is : company_id, industry, revenu, employees, job30 + #job_submitted, job90 + #job_submitted, monthlyFee, totalFee","title":"to rework"},{"location":"rt-analytics/#current-error","text":"2023 - 01 - 04 21 : 43 : 34 com . esotericsoftware . kryo . KryoException : Unable to find class : software . amazon . awssdk . internal . http . LowCopyListMap$$Lambda$1273 / 0x0000000800edc840 Serialization trace : mapConstructor ( software . amazon . awssdk . internal . http . LowCopyListMap$ForBuilder ) headers ( software . amazon . awssdk . http . DefaultSdkHttpFullRequest$Builder ) builder ( jbcodeforce . bgdatajob . operators . Sig4SignedHttpRequestAsyncFunction$HttpRequest ) at com . esotericsoftware . kryo . util . DefaultClassResolver . readName ( DefaultClassResolver . java : 138 ) at com . esotericsoftware . kryo . util . DefaultClassResolver . readClass ( DefaultClassResolver . java : 115 )","title":"Current error"}]}